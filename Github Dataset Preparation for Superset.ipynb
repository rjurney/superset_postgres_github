{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Github Preparation for Superset Examples\n",
    "\n",
    "This notebook will process complex semi-structured JSON records from the [Github Archive](http://gharchive.org) into relational tables in the form of Parquet files for loading into Redshift.\n",
    "\n",
    "**TODO**: Add public boolean fields to all records that have them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>8</td><td>application_1555965269403_0009</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-57-162.us-west-2.compute.internal:20888/proxy/application_1555965269403_0009/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-57-162.us-west-2.compute.internal:8042/node/containerlogs/container_1555965269403_0009_01_000001/livy\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "(<SparkContext master=yarn appName=livy-session-8>, <pyspark.sql.session.SparkSession object at 0x7f5918369f60>)"
     ]
    }
   ],
   "source": [
    "import sys, os, re\n",
    "import json\n",
    "\n",
    "from dateutil.parser import parse as duparse\n",
    "\n",
    "sc, spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the pile of JSON events of all types from gharchive.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all Github events for the year spanning 04-01-2018 to 03-31-2019\n",
    "github_lines = sc.textFile('s3://github-dataset/*.json.gz') # Change me to entire bucket!\n",
    "\n",
    "# Apply the function to every record\n",
    "def parse_json(line):\n",
    "    record = None\n",
    "    try:\n",
    "        record = json.loads(line)\n",
    "    except json.JSONDecodeError as e:\n",
    "        sys.stderr.write(str(e))\n",
    "        record = {'error': 'Parse error'}\n",
    "    return record\n",
    "\n",
    "github_events = github_lines.map(parse_json)\n",
    "github_events = github_events.filter(lambda x: 'error' not in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ForkEvent', 'CreateEvent', 'IssueCommentEvent', 'PushEvent', 'PushEvent', 'PushEvent', 'CreateEvent', 'PushEvent', 'PullRequestEvent', 'PushEvent']"
     ]
    }
   ],
   "source": [
    "# Note there are all kinds of events in the pile\n",
    "[x['type'] for x in github_events.take(10) if 'type' in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the events into types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our events out by type\n",
    "split_types = lambda x, t: 'type' in x and x['type'] == t \n",
    "\n",
    "# See https://developer.github.com/v3/activity/events/types/\n",
    "create_events         = github_events.filter(lambda x: split_types(x, 'CreateEvent'))\n",
    "delete_events         = github_events.filter(lambda x: split_types(x, 'DeleteEvent'))\n",
    "fork_events           = github_events.filter(lambda x: split_types(x, 'ForkEvent'))\n",
    "gist_events           = github_events.filter(lambda x: split_types(x, 'GistEvent'))\n",
    "issue_events          = github_events.filter(lambda x: split_types(x, 'IssuesEvent'))\n",
    "issue_comment_events  = github_events.filter(lambda x: split_types(x, 'IssueCommentEvent'))\n",
    "member_events         = github_events.filter(lambda x: split_types(x, 'MemberEvent'))\n",
    "push_events           = github_events.filter(lambda x: split_types(x, 'PushEvent'))\n",
    "pull_events           = github_events.filter(lambda x: split_types(x, 'PullRequestEvent'))\n",
    "repo_events           = github_events.filter(lambda x: split_types(x, 'RepositoryEvent'))\n",
    "star_events           = github_events.filter(lambda x: split_types(x, 'StarEvent'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check: did it work? - may have to run more than once...\n",
    "# [(x[0], x[1]['type']) for x in [\n",
    "#     ('CreateEvent', create_events.first()), \n",
    "#     ('DeleteEvent', delete_events.first()), \n",
    "#     ('ForkEvent',   fork_events.first())\n",
    "# ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract a `Fork` table from `ForkEvents` and persist as Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# See: https://developer.github.com/v3/activity/events/types/#forkevent\n",
    "def extract_fork(f):\n",
    "    \"\"\"Extracts Rows of ForkEvents and their associated fields from a gharchive.org event dict\"\"\"\n",
    "    \n",
    "    # Out forks...\n",
    "    out_f = {\n",
    "        'id': f['id'],\n",
    "        'created_at': duparse(f['created_at']),\n",
    "        'type': 'ForkEvent',\n",
    "        'public': f['public']\n",
    "    }\n",
    "    \n",
    "    actor = f['actor']\n",
    "    out_f['actor_user_id'] = actor['id']\n",
    "    out_f['actor_user_name'] = actor['login']\n",
    "    \n",
    "    org = f['org'] if 'org' in f else {}\n",
    "    out_f['from_org_id'] = org['id'] if 'id' in org else ''\n",
    "    out_f['from_org_login'] = org['login'] if 'login' in org else ''\n",
    "    \n",
    "    repo = f['repo']\n",
    "    out_f['from_repo_id'] = repo['id']\n",
    "    out_f['from_repo_name'] = repo['name']\n",
    "    \n",
    "    payload = f['payload']\n",
    "    forkee = payload['forkee']\n",
    "    \n",
    "    owner = forkee['owner']\n",
    "    out_f['to_user_id'] = owner['id']\n",
    "    out_f['to_user_name'] = owner['login']\n",
    "    \n",
    "    out_f['to_repo_created_at'] = duparse(forkee['created_at'])\n",
    "    out_f['to_repo_updated_at'] = duparse(forkee['updated_at'])\n",
    "    out_f['to_repo_pushed_at'] = duparse(forkee['pushed_at'])\n",
    "    \n",
    "    out_f['to_repo_size'] = forkee['size']\n",
    "    out_f['to_repo_stargazer_count'] = forkee['stargazers_count']\n",
    "    out_f['to_repo_watcher_count'] = forkee['watchers_count']\n",
    "    out_f['to_repo_forks_count'] = forkee['forks_count']\n",
    "    \n",
    "    license = forkee['license'] if 'license' in forkee and isinstance(forkee['license'], dict) else {}\n",
    "    out_f['to_license_key'] = license['key'] if 'key' in license else ''\n",
    "    out_f['to_license_name'] = license['name'] if 'name' in license else ''\n",
    "    \n",
    "    return Row(**out_f)\n",
    "\n",
    "forks = fork_events.map(extract_fork).toDF().select(\n",
    "    'id',\n",
    "    'type',\n",
    "    'created_at',\n",
    "    'actor_user_id',\n",
    "    'actor_user_name',\n",
    "    'from_org_id',\n",
    "    'from_org_login',\n",
    "    'to_user_id',\n",
    "    'to_user_name',\n",
    "    'to_repo_created_at',\n",
    "    'to_repo_updated_at',\n",
    "    'to_repo_pushed_at',\n",
    "    'to_repo_size',\n",
    "    'to_repo_stargazer_count',\n",
    "    'to_repo_watcher_count',\n",
    "    'to_repo_forks_count',\n",
    "    'to_license_key',\n",
    "    'to_license_name',\n",
    "    'public'\n",
    ")\n",
    "forks.write.mode('overwrite').parquet('s3://github-superset-parquet/ForkEvents.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forks = spark.read.parquet('s3://github-superset-parquet/ForkEvents.parquet')\n",
    "forks.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract `Push` and `Commit` tables from `PushEvents` and persist as Parquet\n",
    "\n",
    "This is a little more complicated as there are two tables to extract: `Push` records that link to one or more `Commit` records. We [`RDD.flatMap()`](https://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=flatmap#pyspark.RDD.flatMap) `extract_push(p)` to create an `RDD[Row]` of both event types and then we split them based on the type into distinct `DataFrames`, which we persist as Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See: https://developer.github.com/v3/activity/events/types/#pushevent\n",
    "def extract_push(p):\n",
    "    \"\"\"Extracts Rows of PushEvents and their associated Commits from a gharchive.org event dict\"\"\"\n",
    "    \n",
    "    # Out pushes...\n",
    "    out_p = {\n",
    "        'type': 'PushEvent',\n",
    "        'id': p['id'],\n",
    "        'created_at': duparse(p['created_at']),\n",
    "        'public': p['public']\n",
    "    }\n",
    "    \n",
    "    # Who pushed it?\n",
    "    actor = p['actor']\n",
    "    out_p['actor_id'] = actor['id']\n",
    "    out_p['actor_user_name'] = actor['login']\n",
    "    \n",
    "    # To what repo?\n",
    "    repo = p['repo']\n",
    "    out_p['repo_id'] = repo['id']\n",
    "    out_p['repo_name'] = repo['name']\n",
    "    \n",
    "    # What did they push?\n",
    "    payload = p['payload']\n",
    "    out_p['push_id'] = payload['push_id']\n",
    "    out_p['push_size'] = payload['size']\n",
    "    out_p['push_ref'] = payload['ref']\n",
    "    out_p['push_head'] = payload['head']\n",
    "    out_p['push_before'] = payload['before']\n",
    "    \n",
    "    # Out commits...\n",
    "    out_cs = []\n",
    "    commits = payload['commits']\n",
    "    for c in commits:\n",
    "        out_c = {\n",
    "            'type': 'Commit',\n",
    "            'sha': c['sha'],\n",
    "            'repo_id': out_p['repo_id'],\n",
    "            'repo_name': out_p['repo_name'],\n",
    "            'push_id': out_p['push_id'],\n",
    "            'actor_id': out_p['actor_id'],\n",
    "            'actor_user_name': out_p['actor_user_name'],\n",
    "            'author_name': c['author']['name'],\n",
    "            'url': c['url'],\n",
    "            'message': c['message'],\n",
    "            'push_created_at': out_p['created_at'],\n",
    "            'public': out_p['public']\n",
    "        }\n",
    "        out_cs.append(Row(**out_c))\n",
    "\n",
    "    return [Row(**out_p)] + (out_cs)\n",
    "\n",
    "# Generate both PushEvents and Commits in varyin length lists with flatMap...\n",
    "push_and_commits = push_events.flatMap(extract_push)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split pushes, make DataFrame and store as CSV for a SQL DB\n",
    "pushes_raw = push_and_commits.filter(lambda x: x['type'] == 'PushEvent')\n",
    "pushes = pushes_raw.toDF().select(\n",
    "    'id',\n",
    "    'type',\n",
    "    'actor_id',\n",
    "    'actor_user_name',\n",
    "    'repo_id',\n",
    "    'repo_name',\n",
    "    'push_id',\n",
    "    'push_size',\n",
    "    'push_ref',\n",
    "    'push_head',\n",
    "    'push_before',\n",
    "    'created_at',\n",
    "    'public'\n",
    ")\n",
    "pushes.write.mode('overwrite').parquet('s3://github-superset-parquet/PushEvents.Parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pushes = spark.read.parquet('s3://github-superset-parquet/PushEvents.Parquet')\n",
    "pushes.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split commits, make DataFrame and store as CSV for a SQL DB\n",
    "commits_raw = push_and_commits.filter(lambda x: x['type'] == 'Commit')\n",
    "commits = commits_raw.toDF().select(\n",
    "    'sha',\n",
    "    'type',\n",
    "    'push_id',\n",
    "    'actor_id',\n",
    "    'repo_id',\n",
    "    'repo_name',\n",
    "    'actor_user_name',\n",
    "    'author_name',\n",
    "    'url',\n",
    "    'message',\n",
    "    'push_created_at',\n",
    "    'public'\n",
    ")\n",
    "commits.write.mode('overwrite').parquet('s3://github-superset-parquet/Commits.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commits = spark.read.parquet('s3://github-superset-parquet/Commits.parquet')\n",
    "commits.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract a `Create` table from `CreateEvents` and persist as Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See https://developer.github.com/v3/activity/events/types/#createvent\n",
    "def extract_create(c):\n",
    "    \"\"\"Extract Rows of CreateEvents and their associated fields from a gharchive.org event dict\"\"\"\n",
    "    \n",
    "    # Out creates...\n",
    "    out_c = {\n",
    "        'id': c['id'],\n",
    "        'created_at': duparse(c['created_at']),\n",
    "        'type': 'CreateEvent',\n",
    "    }\n",
    "    \n",
    "    actor = c['actor']\n",
    "    out_c['actor_id'] = actor['id']\n",
    "    out_c['actor_user_name'] = actor['login']\n",
    "    \n",
    "    repo = c['repo']\n",
    "    out_c['repo_id'] = repo['id']\n",
    "    out_c['repo_name'] = repo['name']\n",
    "    \n",
    "    out_c['public'] = c['public']\n",
    "    \n",
    "    return Row(**out_c)\n",
    "\n",
    "create_events.map(extract_create).toDF().select(\n",
    "    'id',\n",
    "    'type',\n",
    "    'created_at',\n",
    "    'actor_id',\n",
    "    'actor_user_name',\n",
    "    'repo_id',\n",
    "    'repo_name',\n",
    "    'public'\n",
    ").write.mode('overwrite').parquet('s3://github-superset-parquet/Creates.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creates = spark.read.parquet('s3://github-superset-parquet/Creates.parquet')\n",
    "creates.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract `Delete` tables from `DeleteEvents` and persist as Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See https://developer.github.com/v3/activity/events/types/#deleteevent\n",
    "def extract_delete(d):\n",
    "    \"\"\"Extract Rows of DeleteEvents and their associated fields from a gharchive.org event dict\"\"\"\n",
    "    \n",
    "    # Out deletes...\n",
    "    out_d = {\n",
    "        'id': d['id'],\n",
    "        'type': 'DeleteEvent',\n",
    "        'created_at': duparse(d['created_at'])\n",
    "    }\n",
    "    \n",
    "    actor = d['actor']\n",
    "    out_d['actor_id'] = actor['id']\n",
    "    out_d['actor_user_name'] = actor['login']\n",
    "    \n",
    "    repo = d['repo']\n",
    "    out_d['repo_id'] = repo['id']\n",
    "    out_d['repo_name'] = repo['name']\n",
    "    \n",
    "    org = d['org'] if 'org' in d else {}\n",
    "    out_d['org_id'] = org['id'] if 'id' in org else ''\n",
    "    out_d['org_name'] = org['login'] if 'login' in org else ''\n",
    "    \n",
    "    out_d['public'] = d['public']\n",
    "    \n",
    "    return Row(**out_d)\n",
    "\n",
    "delete_events.map(extract_delete).toDF().select(\n",
    "    'id',\n",
    "    'type',\n",
    "    'created_at',\n",
    "    'actor_id',\n",
    "    'actor_user_name',\n",
    "    'repo_id',\n",
    "    'repo_name',\n",
    "    'org_id',\n",
    "    'org_name',\n",
    "    'public'\n",
    ").write.mode('overwrite').parquet('s3://github-superset-parquet/Deletes.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deletes = spark.read.parquet('s3://github-superset-parquet/Deletes.parquet')\n",
    "deletes.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract an `Issue` table from `IssuesEvents` and persist as Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See https://developer.github.com/v3/activity/events/types/#issueevent\n",
    "def extract_issue(i):\n",
    "    \"\"\"Extract Rows of IssueEvents and their associated fields from a gharchive.org event dict\"\"\"\n",
    "    \n",
    "    # Out issues...\n",
    "    out_i = {\n",
    "        'id': i['id'],\n",
    "        'type': 'IssuesEvent',\n",
    "        'created_at': duparse(i['created_at']),\n",
    "        'public': i['public']\n",
    "    }\n",
    "    \n",
    "    actor = i['actor']\n",
    "    out_i['actor_id'] = actor['id']\n",
    "    out_i['actor_user_name'] = actor['login']\n",
    "    \n",
    "    repo = i['repo']\n",
    "    out_i['repo_id'] = repo['id']\n",
    "    out_i['repo_name'] = repo['name']\n",
    "    \n",
    "    payload = i['payload']\n",
    "    out_i['action'] = payload['action']\n",
    "\n",
    "    issue = payload['issue']\n",
    "    out_i['assignee'] = issue['assignee']\n",
    "    out_i['assignees'] = issue['assignees']\n",
    "    out_i['body'] = issue['body']\n",
    "    out_i['closed_at'] = duparse(issue['closed_at']) if issue['closed_at'] else None\n",
    "    out_i['comments'] = issue['comments']\n",
    "    out_i['issue_id'] = issue['id']\n",
    "    out_i['labels'] = issue['labels']\n",
    "    out_i['locked'] = issue['locked']\n",
    "    out_i['number'] = issue['number']\n",
    "    out_i['title'] = issue['title']\n",
    "    out_i['updated_at'] = issue['updated_at']\n",
    "    \n",
    "    user = issue['user']\n",
    "    out_i['user_id'] = user['id']\n",
    "    out_i['user_name'] = user['login']\n",
    "    \n",
    "    return Row(**out_i)\n",
    "\n",
    "issue_events.map(extract_issue).toDF().select(\n",
    "    'id',\n",
    "    'type',\n",
    "    'created_at',\n",
    "    'updated_at',\n",
    "    'closed_at',\n",
    "    'actor_id',\n",
    "    'actor_user_name',\n",
    "    'repo_id',\n",
    "    'repo_name',\n",
    "    'user_id',\n",
    "    'user_name',\n",
    "    'action',\n",
    "    'assignee',\n",
    "    'assignees',\n",
    "    'title',\n",
    "    'body',\n",
    "    'comments',\n",
    "    'issue_id',\n",
    "    'labels',\n",
    "    'locked',\n",
    "    'number',\n",
    "    'public'\n",
    ").write.mode('overwrite').parquet('s3://github-superset-parquet/Issues.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues = spark.read.parquet('s3://github-superset-parquet/Issues.parquet')\n",
    "issues.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract an `IssueComment` table from `IssueCommentEvents` and persist as Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # See https://developer.github.com/v3/activity/events/types/#issuecommentevent\n",
    "# def extract_issue_comment(i):\n",
    "#     \"\"\"Extract Rows of IssueCommentEvents and their associated fields from a gharchive.org event dict\"\"\"\n",
    "    \n",
    "#     # Out issue comments...\n",
    "#     out_i = {\n",
    "#         'id': i['id'],\n",
    "#         'created_at': duparse(i['created_at'])\n",
    "#     }\n",
    "    \n",
    "#     return i\n",
    "\n",
    "# issue_comment_events.map(extract_issue_comment).take(10)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See https://developer.github.com/v3/activity/events/types/#memberevent\n",
    "def extract_member(m):\n",
    "    \"\"\"Extract Rows of MemberEvents and their associated fields from a gharchive.org event dict\"\"\"\n",
    "    \n",
    "    # Out members...\n",
    "    out_m = {\n",
    "        'id': m['id'],\n",
    "        'type': 'MemberEvent',\n",
    "        'created_at': duparse(m['created_at']),\n",
    "        'public': m['public']\n",
    "    }\n",
    "    \n",
    "    actor = m['actor']\n",
    "    out_m['actor_id'] = actor['id']\n",
    "    out_m['actor_user_name'] = actor['login']\n",
    "    \n",
    "    payload = m['payload']\n",
    "    out_m['action'] = payload['action']\n",
    "\n",
    "    member = payload['member']\n",
    "    out_m['member_id'] = member['id']\n",
    "    out_m['member_name'] = member['login']\n",
    "    out_m['site_admin'] = member['site_admin']\n",
    "    \n",
    "    repo = m['repo']\n",
    "    out_m['repo_id'] = repo['id']\n",
    "    out_m['repo_name'] = repo['name']\n",
    "    \n",
    "    return out_m\n",
    "\n",
    "member_events.map(extract_member).toDF().select(\n",
    "    'id',\n",
    "    'type',\n",
    "    'created_at',\n",
    "    'actor_id',\n",
    "    'actor_user_name',\n",
    "    'action',\n",
    "    'member_id',\n",
    "    'member_name',\n",
    "    'site_admin',\n",
    "    'repo_id',\n",
    "    'repo_name',\n",
    "    'public'\n",
    ").write.mode('overwrite').parquet('s3://github-superset-parquet/Members.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "members = spark.read.parquet('s3://github-superset-parquet/Members.parquet')\n",
    "members.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See https://developer.github.com/v3/activity/events/types/#pullrequestevent\n",
    "def extract_pull(p):\n",
    "    \"\"\"Extract Rows of PullRequestEvents and their associated fields from a gharchive.org event dict\"\"\"\n",
    "    \n",
    "    # Out pull requests...\n",
    "    out_p = {\n",
    "        'id': p['id'],\n",
    "        'type': 'PullRequestEvent',\n",
    "        'created_at': duparse(p['created_at']),\n",
    "        'public': p['public']\n",
    "    }\n",
    "    \n",
    "    actor = p['actor']\n",
    "    out_p['actor_id'] = actor['id']\n",
    "    out_p['actor_user_name'] = actor['login']\n",
    "    \n",
    "    org = p['org'] if 'org' in p else {}\n",
    "    out_p['org_id'] = org['id'] if 'id' in org else None\n",
    "    out_p['org_name'] = org['login'] if 'login' in org else None\n",
    "    \n",
    "    payload = p['payload']\n",
    "    out_p['action'] = payload['action']\n",
    "    out_p['number'] = payload['number']\n",
    "    \n",
    "    pull_request = payload['pull_request']\n",
    "    out_p['additions'] = pull_request['additions']\n",
    "    out_p['assignee'] = pull_request['assignee']\n",
    "    out_p['assignees'] = pull_request['assignees']\n",
    "    out_p['author_association'] = pull_request['author_association']\n",
    "    \n",
    "    base = pull_request['base']\n",
    "    out_p['base_label'] = base['label']\n",
    "    out_p['base_ref'] = base['ref']\n",
    "    \n",
    "    base_repo = base['repo']\n",
    "    out_p['base_repo_created_at'] = duparse(base_repo['created_at'])\n",
    "    out_p['base_repo_default_branch'] = base_repo['default_branch'] if 'default_branch' in base_repo else None\n",
    "    out_p['base_repo_description'] = base_repo['description']\n",
    "    out_p['base_repo_fork'] = base_repo['fork']\n",
    "    out_p['base_repo_forks'] = base_repo['forks']\n",
    "    out_p['base_repo_full_name'] = base_repo['full_name']\n",
    "    out_p['base_repo_id'] = base_repo['id']\n",
    "    out_p['base_repo_language'] = base_repo['language']\n",
    "    \n",
    "    license = base_repo['license'] if isinstance(base_repo['license'], dict) else {}\n",
    "    out_p['base_repo_license_key'] = license['key'] if 'key' in license else None\n",
    "    out_p['base_repo_license_name'] = license['name'] if 'name' in license else None\n",
    "    \n",
    "    out_p['base_repo_name'] = base_repo['name']\n",
    "    out_p['base_repo_open_issues'] = base_repo['open_issues']\n",
    "    \n",
    "    owner = base_repo['owner']\n",
    "    out_p['base_repo_owner_id'] = owner['id']\n",
    "    out_p['base_repo_owner_user_name'] = owner['login']\n",
    "    out_p['base_repo_owner_site_admin'] = owner['site_admin']\n",
    "    \n",
    "    out_p['base_repo_private'] = base_repo['private']\n",
    "    out_p['base_repo_pushed_at'] = duparse(base_repo['pushed_at'])\n",
    "    out_p['base_repo_size'] = base_repo['size']\n",
    "    out_p['base_repo_stargazers_count'] = base_repo['stargazers_count']\n",
    "    out_p['base_repo_updated_at'] = duparse(base_repo['updated_at'])\n",
    "    out_p['base_repo_watchers'] = base_repo['watchers']\n",
    "    \n",
    "    out_p['base_sha'] = base['sha']\n",
    "    \n",
    "    base_user = base['user']\n",
    "    out_p['base_user_id'] = base_user['id']\n",
    "    out_p['base_user_user_name'] = base_user['login']\n",
    "    out_p['base_user_site_admin'] = base_user['site_admin']\n",
    "\n",
    "    out_p['body'] = pull_request['body']\n",
    "    out_p['changed_files'] = pull_request['changed_files']\n",
    "    out_p['closed_at'] = duparse(pull_request['closed_at']) if pull_request['closed_at'] else None\n",
    "    out_p['comments'] = pull_request['comments']\n",
    "    out_p['commits'] = pull_request['commits']\n",
    "    out_p['created_at'] = duparse(pull_request['created_at'])\n",
    "    out_p['deletions'] = pull_request['deletions']\n",
    "    \n",
    "    head = pull_request['head']\n",
    "    out_p['head_label'] = head['label']\n",
    "    out_p['head_ref'] = head['ref']\n",
    "    \n",
    "    head_repo = head['repo'] if 'repo' in head and isinstance(head['repo'], dict) else {}\n",
    "    out_p['head_repo_created_at'] = duparse(head_repo['created_at']) if 'created_at' in head_repo and head_repo['created_at'] else None\n",
    "    out_p['head_repo_default_branch'] = head_repo['default_branch'] if 'default_branch' in head_repo else None\n",
    "    out_p['head_repo_description'] = head_repo['description'] if 'description' in head_repo else None\n",
    "    out_p['head_repo_fork'] = head_repo['fork'] if 'fork' in head_repo else None\n",
    "    out_p['head_repo_forks'] = head_repo['forks'] if 'forks' in head_repo else None\n",
    "    out_p['head_repo_full_name'] = head_repo['full_name'] if 'full_name' in head_repo else None\n",
    "    out_p['head_repo_id'] = head_repo['id'] if 'id' in head_repo else None\n",
    "    out_p['head_repo_language'] = head_repo['language'] if 'language' in head_repo else None\n",
    "    out_p['head_repo_languages'] = head_repo['languages'] if 'languages' in head_repo else ''\n",
    "    \n",
    "    head_repo_license = head_repo['license'] if 'license' in head_repo and isinstance(head_repo['license'], dict) else {}\n",
    "    out_p['head_repo_license_key'] = head_repo_license['key'] if 'key' in head_repo_license else None\n",
    "    out_p['head_repo_license_name'] = head_repo_license['name'] if 'name' in head_repo_license else None\n",
    "    \n",
    "    out_p['head_repo_name'] = head_repo['name'] if 'name' in head_repo else None\n",
    "    out_p['head_repo_open_issues'] = head_repo['open_issues'] if 'open_issues' in head_repo else None\n",
    "    \n",
    "    head_repo_owner = head_repo['owner'] if 'owner' in head_repo else {}\n",
    "    out_p['head_repo_owner_id'] = head_repo_owner['id'] if 'id' in head_repo_owner else None\n",
    "    out_p['head_repo_owner_user_name'] = head_repo_owner['login'] if 'login' in head_repo_owner else None\n",
    "    out_p['head_repo_owner_site_admin'] = head_repo_owner['site_admin'] if 'site_admin' in head_repo_owner else None\n",
    "    \n",
    "    out_p['head_repo_private'] = head_repo['private'] if 'private' in head_repo else None\n",
    "    out_p['head_repo_pushed_at'] = duparse(head_repo['pushed_at']) if 'pushed_at' in head_repo else None\n",
    "    out_p['head_repo_size'] = head_repo['size'] if 'size' in head_repo else None\n",
    "    out_p['head_repo_stargazers_count'] = head_repo['stargazers_count'] if 'stargazers_count' in head_repo else None\n",
    "    out_p['head_repo_updated_at'] = duparse(head_repo['updated_at']) if 'updated_at' in head_repo else None\n",
    "    out_p['head_repo_watchers'] = head_repo['watchers'] if 'watchers' in head_repo else None\n",
    "    \n",
    "    out_p['head_sha'] = head['sha']\n",
    "\n",
    "    head_user = head['user']\n",
    "    out_p['head_user_id'] = head_user['id']\n",
    "    out_p['head_user_name'] = head_user['login']\n",
    "    out_p['head_user_site_admin'] = head_user['site_admin']\n",
    "    \n",
    "    out_p['id'] = pull_request['id']\n",
    "    out_p['labels'] = pull_request['labels']\n",
    "    out_p['locked'] = pull_request['locked']\n",
    "    out_p['merge_commit_sha'] = pull_request['merge_commit_sha']\n",
    "    out_p['mergeable'] = pull_request['mergeable']\n",
    "    out_p['merged'] = pull_request['merged']\n",
    "    out_p['merged_at'] = duparse(pull_request['merged_at']) if 'merged_at' in pull_request and pull_request['merged_at'] else None\n",
    "    out_p['merged_by'] = pull_request['merged_by']\n",
    "    out_p['milestone'] = pull_request['milestone']\n",
    "    out_p['number'] = pull_request['number']\n",
    "    out_p['rebaseable'] = pull_request['rebaseable']\n",
    "    out_p['requested_reviewers'] = pull_request['requested_reviewers']\n",
    "    out_p['requested_teams'] = pull_request['requested_teams']\n",
    "    out_p['review_comments'] = pull_request['review_comments']\n",
    "    out_p['state'] = pull_request['state']\n",
    "    out_p['title'] = pull_request['title']\n",
    "    out_p['updated_at'] = duparse(pull_request['updated_at']) if 'updated_at' in pull_request else None\n",
    "    \n",
    "    user = pull_request['user']\n",
    "    out_p['user_id'] = user['id']\n",
    "    out_p['user_name'] = user['login']\n",
    "    out_p['user_site_admin'] = user['site_admin']\n",
    "    \n",
    "    out_p['public'] = p['public']\n",
    "    \n",
    "    repo = p['repo']\n",
    "    out_p['repo_id'] = repo['id']\n",
    "    out_p['repo_name'] = repo['name']\n",
    "    \n",
    "    return out_p\n",
    "\n",
    "pull_events.map(extract_pull).toDF().write.mode('overwrite').parquet(\n",
    "    's3://github-superset-parquet/PullRequests.parquet'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pull_requests = spark.read.parquet('s3://github-superset-parquet/PullRequests.parquet')\n",
    "pull_requests.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See https://developer.github.com/v3/activity/events/types/#repositoryevent\n",
    "def extract_repo(r):\n",
    "    \"\"\"Extract Rows of RepositoryEvents and their associated fields from a gharchive.org event dict\"\"\"\n",
    "    \n",
    "    # Our repos...\n",
    "    out_r = {\n",
    "        'id': r['id'],\n",
    "        'created_at': duparse(r['created_at'])\n",
    "    }\n",
    "    \n",
    "    return r\n",
    "\n",
    "repo_events.map(extract_repo).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See https://developer.github.com/v3/activity/events/types/#watchevent\n",
    "def extract_star(s):\n",
    "    \"\"\"Extract Rows of WatchEvents and their associated fields from a gharchive.org event dict\"\"\"\n",
    "    \n",
    "    # Out stars...\n",
    "    out_s = {\n",
    "        'id': s['id'],\n",
    "        'created_at': s['created_at']\n",
    "    }\n",
    "    \n",
    "    return s\n",
    "\n",
    "star_events.map(extract_star).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
